{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = np.loadtxt(\"mnist_train.csv\", \n",
    "                        delimiter=\",\")\n",
    "test_data = np.loadtxt(\"mnist_test.csv\", \n",
    "                       delimiter=\",\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_one_hot(n):\n",
    "    one_hot = [0]*10\n",
    "    one_hot[n] = 1\n",
    "    return one_hot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_labels = test_data[:, 0].astype(int)\n",
    "test_labels_one_hot = np.asarray([to_one_hot(int(x.tolist())) for x in test_labels])\n",
    "test_images = test_data[:, 1:].reshape(-1, 28,28) / 255.\n",
    "train_labels = train_data[:, 0].astype(int)\n",
    "train_labels_one_hot = np.asarray([to_one_hot(int(x.tolist())) for x in train_labels])\n",
    "train_images = train_data[:, 1:].reshape(-1, 28,28) / 255."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_labels_one_hot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearLayer:\n",
    "    def __init__(self, in_n, out_n):\n",
    "        self.in_n = in_n\n",
    "        self.out_n = out_n\n",
    "        # Kaiming inizialization\n",
    "        self.b = np.zeros((1, out_n))\n",
    "        self.W = np.random.randn(in_n, out_n) * np.sqrt(2/in_n)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        self.minibatch_n = x.shape[0]\n",
    "        # dim x = minibatch_n x in_n\n",
    "        self.x = x\n",
    "        # minibatch_n x out_n\n",
    "        y = np.matmul(x, self.W) + self.b # broadcasting su b\n",
    "        return y\n",
    "        \n",
    "    def backward(self, dy):          \n",
    "        # dim dy = minibatch_n x out_n\n",
    "        # minibatch_n x in_n\n",
    "        dx = np.matmul(dy, self.W.T)\n",
    "        # minibatch_n x out_n\n",
    "        self.db = dy\n",
    "        self.db = np.sum(self.db, axis=0)\n",
    "        # minibatch_n x in_n x out_n\n",
    "        self.dW = np.matmul(self.x.reshape(self.minibatch_n, self.in_n, 1), dy.reshape(self.minibatch_n, 1, self.out_n))\n",
    "        self.dW = np.sum(self.dW, axis=0)\n",
    "        return dx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReLU:\n",
    "    def forward(self, x):\n",
    "        self.mask = x > 0\n",
    "        return np.maximum(x, 0)\n",
    "    \n",
    "    def backward(self, dy):\n",
    "        return np.multiply(dy, self.mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Sigmoid:\n",
    "    def forward(self, x):\n",
    "        self.sigma = 1/(1+np.exp(-x))\n",
    "        return self.sigma\n",
    "    \n",
    "    def backward(self, dy):\n",
    "        return np.multiply(dy, np.multiply(self.sigma, 1 - self.sigma))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Tanh:\n",
    "    def forward(self, x):\n",
    "        self.tanh = np.tanh(x)\n",
    "        return self.tanh\n",
    "    \n",
    "    def backward(self, dy):\n",
    "        return np.multiply(dy, 1 - self.tanh**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Identity:\n",
    "    def forward(self, x):\n",
    "        return x\n",
    "    \n",
    "    def backward(self, dy):\n",
    "        return dy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Softmax:\n",
    "    def forward(self, x):\n",
    "        # dim x = batch x n\n",
    "        M = np.amax(x, axis=1, keepdims=True)\n",
    "        # per evitare overflow\n",
    "        # batch x n\n",
    "        self.expx = np.exp(np.subtract(x, M))\n",
    "        # batch x 1\n",
    "        self.expsums = np.sum(self.expx, axis=1, keepdims=True)\n",
    "        return np.divide(self.expx, self.expsums)\n",
    "        \n",
    "    def backward(self, dy):\n",
    "        # dim dy = batch x n\n",
    "        # dx = np.zeros_like(dy)\n",
    "        b, n = dy.shape\n",
    "        expnorm = self.expx/self.expsums\n",
    "        J = -expnorm.reshape(b,n,1)@expnorm.reshape(b,1,n) + expnorm.reshape(b,1,n)*np.identity(n).reshape(1,n,n)\n",
    "        # batch x n\n",
    "        dx = np.matmul(dy.reshape(b, 1, n), J).reshape(b, n)\n",
    "        return dx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CrossEntropyLoss:\n",
    "    def forward(self, y, y_):\n",
    "        # y sono le vere label\n",
    "        # y_ sono le label predette\n",
    "        # dim y : b x n\n",
    "        self.b = y.shape[0]\n",
    "        return - (y*np.log(y_)).sum()/self.b\n",
    "        \n",
    "    def backward(self, y, y_):\n",
    "        # b x n\n",
    "        dy_ = -1/self.b * y/y_\n",
    "        return dy_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP:\n",
    "    def __init__(self, layers_n, activation=\"ReLU\"):\n",
    "        self.activations_dict = {\"ReLU\" : ReLU, \"Tanh\" : Tanh, \"Sigmoid\" : Sigmoid, \"Identity\" : Identity}\n",
    "        self.layers_n = layers_n\n",
    "        self.N = len(layers_n) - 1\n",
    "        self.linear_layers = [LinearLayer(layers_n[i], layers_n[i+1]) for i in range(self.N)]\n",
    "        self.activations = [self.activations_dict[activation]() for i in range(self.N-1)]\n",
    "        self.softmax = Softmax()\n",
    "        self.loss = CrossEntropyLoss()\n",
    "        \n",
    "    def forward(self, x, y):\n",
    "        for i in range(self.N-1):\n",
    "            x = self.linear_layers[i].forward(x)\n",
    "            x = self.activations[i].forward(x)\n",
    "        y_ = self.linear_layers[self.N-1].forward(x)\n",
    "        y_ = self.softmax.forward(y_)\n",
    "        loss = self.loss.forward(y, y_)\n",
    "        return y_, loss\n",
    "\n",
    "    def backward(self, y, y_):\n",
    "        dj = self.loss.backward(y, y_)\n",
    "        dj = self.softmax.backward(dj)\n",
    "        linear_layer = self.linear_layers[-1]\n",
    "        dj = linear_layer.backward(dj)\n",
    "        for linear_layer, activation in zip(list(reversed(self.linear_layers))[1:], reversed(self.activations)):\n",
    "            dj = activation.backward(dj)\n",
    "            dj = linear_layer.backward(dj)\n",
    "            \n",
    "    def gradient_descent_step(self, alpha):\n",
    "        for linear_layer in self.linear_layers:\n",
    "            linear_layer.W -= alpha*linear_layer.dW\n",
    "            linear_layer.b -= alpha*linear_layer.db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn = MLP([28*28, 100, 100, 100, 10], activation=\"ReLU\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_random_patch(x, y, n):\n",
    "    idx = np.asarray(random.sample(range(x.shape[0]), n))\n",
    "    return x[idx], y[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_accuracy(y, y_):\n",
    "    y__ = np.zeros_like(y_)\n",
    "    y__[np.arange(y_.shape[0]), np.argmax(y_, axis=1)] = 1\n",
    "    ret = np.all(y == y__, axis=1)\n",
    "    return ret.sum()/ret.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_losses = []\n",
    "test_losses = []\n",
    "accuracies = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_EPOCHS = 5\n",
    "ALPHA = 0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "xs = np.array_split(train_images, 1_000)\n",
    "ys = np.array_split(train_labels_one_hot, 1_000)\n",
    "for epoch in range(1, NUM_EPOCHS+1):\n",
    "    for x, y in zip(xs, ys):\n",
    "        y_, train_loss = nn.forward(x.reshape(-1, 28*28), y)\n",
    "        nn.backward(y, y_)\n",
    "        nn.gradient_descent_step(ALPHA)\n",
    "        # random batch dal test set\n",
    "        batch_images, batch_labels_one_hot = get_random_patch(test_images, test_labels_one_hot, 1000)\n",
    "        y_, test_loss = nn.forward(batch_images.reshape(-1, 28*28), batch_labels_one_hot)\n",
    "        # accuratezza sul test set\n",
    "        acc = get_accuracy(batch_labels_one_hot, y_)\n",
    "        accuracies.append(acc)\n",
    "        train_losses.append(train_loss)\n",
    "        test_losses.append(test_loss)\n",
    "\n",
    "        print(f'epoch = {epoch}, train_loss = {train_loss:.2f}, test_accuracy={acc:.2f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}